{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ec479a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ensin\\anaconda3\\envs\\ResearchTopics\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from torch.utils.data import Dataset\n",
    "from math import floor, ceil\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
    "import torchvision\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import pandas as pd \n",
    "import shutil\n",
    "import os\n",
    "import tarfile\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d286df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_bird_images(csv_file, image_dir, output_dir):\n",
    "    \"Organizes images from the Waterbirds dataset into two classes: landbirds and waterbirds\"\n",
    "    # Create output directories for landbirds and waterbirds\n",
    "    landbird_dir = os.path.join(output_dir, 'landbirds')\n",
    "    waterbird_dir = os.path.join(output_dir, 'waterbirds')\n",
    "\n",
    "    os.makedirs(landbird_dir, exist_ok=True)\n",
    "    os.makedirs(waterbird_dir, exist_ok=True)\n",
    "\n",
    "    # Read the CSV file which stores the image filenames with their image labels\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        image_filename = row['img_filename']\n",
    "        label = row['y'] \n",
    "\n",
    "        image_name = os.path.basename(image_filename)\n",
    "\n",
    "        # Define the destination folder based on the label\n",
    "        if label == 0:\n",
    "            destination = os.path.join(landbird_dir, image_name)\n",
    "        elif label == 1:\n",
    "            destination = os.path.join(waterbird_dir, image_name)\n",
    "        else:\n",
    "            print(f\"Invalid label for {image_name}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        source_path = os.path.join(image_dir, image_filename)\n",
    "\n",
    "        # Copy or move the image to the appropriate folder\n",
    "        shutil.copy(source_path, destination)  # Use shutil.move() to move instead of copy\n",
    "\n",
    "    print(\"Organized images into 'landbirds' and 'waterbirds' folders.\")\n",
    "\n",
    "\n",
    "# csv_file = ''  # Provide the path to your CSV file (metadata.csv)\n",
    "# image_dir = ''  # Provide the path to your image directory (waterbird_complete95_forest2water2 folder)\n",
    "# output_dir = ''  # Specify the output directory\n",
    "# organize_bird_images(csv_file, image_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94ce80e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following implementation is based on the following paper and its corresponding code: \n",
    "# Levy, D., Carmon, Y., Duchi, J. C., & Sidford, A. (2020). \n",
    "# Large-scale methods for distributionally robust optimization. Advances in Neural Information Processing Systems, 33, 8847-8860.\n",
    "\n",
    "def cvar_value(p, v, reg):\n",
    "    \"returns the CVaR value of a distribution given by p and v\"\n",
    "    m = v.shape[0]\n",
    "    idx = torch.nonzero(p) # indices of non-zero elements of p\n",
    "    kl = np.log(m) + (p[idx] * torch.log(p[idx])).sum() # KL divergence is calculated only for non-zero elements of p\n",
    "        \n",
    "    return torch.dot(p.squeeze(), v.squeeze()) - reg * kl\n",
    "\n",
    "class Loss(torch.nn.Module):\n",
    "    \"returns the CVaR loss given by p and v\"\n",
    "    def __init__(self, alpha = 0.1, reg = 0.01, tol = 1e-4):\n",
    "        super(Loss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "        self.reg = reg\n",
    "        # set to cuda if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def response(self, v):\n",
    "        \"function that returns the response of v to the loss class given a certain alpha and reg\"\n",
    "        alpha = self.alpha\n",
    "        reg = self.reg\n",
    "        m = v.shape[0]\n",
    "\n",
    "        # check if reg is larger than zero to avoid division by zero\n",
    "        if reg>0:\n",
    "            \n",
    "            if alpha == 1.0:\n",
    "                return torch.ones_like(v) / m\n",
    "            \n",
    "            def p(eta):\n",
    "                \"function that returns the probability distribution given eta\"\n",
    "                x = (v-eta)/reg\n",
    "                new_x = torch.exp(x)\n",
    "                alph = torch.Tensor([1.0/alpha]).type(x.dtype).to(self.device)\n",
    "                m_tensor = torch.Tensor([m]).type(x.dtype).to(self.device)\n",
    "                # returns the probability distribution as a tensor\n",
    "                return torch.div(torch.min(new_x, alph), m_tensor)\n",
    "            \n",
    "            def bisection_target(eta):\n",
    "                \"function that returns the value of the bisection target as defined in the paper\"\n",
    "                return 1.0 - p(eta).sum()\n",
    "            \n",
    "            # finding eta using bisection method\n",
    "            eta_min = reg * torch.logsumexp(v / reg - np.log(m), 0)\n",
    "            eta_min = eta_min.squeeze()\n",
    "\n",
    "            # if loop to check if the bisection target is smaller than the pre-defined tolerance\n",
    "            if abs(bisection_target(eta_min))<=self.tol:\n",
    "                return p(eta_min)\n",
    "            else:\n",
    "                # fail safe to avoid a None type distribution error\n",
    "                cutoff = int(alpha * m)\n",
    "                surplus = 1.0 - cutoff / (alpha * m)\n",
    "\n",
    "                p = torch.zeros_like(v)\n",
    "                idx = torch.argsort(v, descending=True)\n",
    "                p[idx[:cutoff]] = 1.0 / (alpha * m)\n",
    "                if cutoff < m:\n",
    "                    p[idx[cutoff]] = surplus\n",
    "                return p\n",
    "\n",
    "        else:\n",
    "            print('reg is 0')\n",
    "        \n",
    "    def forward(self, v):\n",
    "        \"returns the CVaR loss given by p and v\"\n",
    "        p = self.response(v)    \n",
    "        return cvar_value(p, v, self.reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33b68cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def visualisation(train_losses, test_losses, average_accuracies, group_accuracies):\n",
    "    \"Visualise the training losses, test losses, average accuracies and group accuracies over the epochs of training\"\n",
    "    # Create a figure with subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "    # Subplot 1 - Plotting training losses\n",
    "    axs[0, 0].plot(range(1, len(train_losses) + 1), train_losses)\n",
    "    axs[0, 0].set_title('Training Loss Per Epoch')\n",
    "    axs[0, 0].set_xlabel('Number of Epochs')\n",
    "    axs[0, 0].set_ylabel('Training Loss')\n",
    "\n",
    "    # Subplot 2 - Plotting test losses\n",
    "    axs[0, 1].plot(range(1, len(test_losses) + 1), test_losses)\n",
    "    axs[0, 1].set_title('Test Loss Per Epoch')\n",
    "    axs[0, 1].set_xlabel('Number of Epochs')\n",
    "    axs[0, 1].set_ylabel('Test Loss')\n",
    "\n",
    "    # Subplot 3 - Plotting average accuracies\n",
    "    axs[1, 0].plot(range(1, len(average_accuracies) + 1), average_accuracies)\n",
    "    axs[1, 0].set_title('Average Accuracy of Test Set Per Epoch')\n",
    "    axs[1, 0].set_xlabel('Number of Epochs')\n",
    "    axs[1, 0].set_ylabel('Average Test Accuracy (%)')\n",
    "\n",
    "    # Subplot 4 - Plotting values in dictionaries for group accuracies\n",
    "    for key in group_accuracies[0].keys():\n",
    "        axs[1, 1].plot(range(1, len(group_accuracies) + 1), [d[key] for d in group_accuracies], label=key+1)\n",
    "\n",
    "    axs[1, 1].set_title('Test Accuracy Per Group Per Epoch')\n",
    "    axs[1, 1].legend()\n",
    "    axs[1, 1].set_xlabel('Number of Epochs')\n",
    "    axs[1, 1].set_ylabel('Accuracy (%)')\n",
    "\n",
    "    # Adjust layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3228597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is based upon the paper and corresponding code:\n",
    "# Liu, E. Z., Haghgoo, B., Chen, A. S., Raghunathan, A., Koh, P. W., Sagawa, S., ... & Finn, C. (2021, July). \n",
    "# Just train twice: Improving group robustness without training group information. In International Conference on Machine Learning (pp. 6781-6792). PMLR.\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"create a custom dataset which stores the path to the image in addition to the image itself and the label\"\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        class_folders = os.listdir(data_dir)\n",
    "        for class_folder in class_folders:\n",
    "            class_path = os.path.join(data_dir, class_folder)\n",
    "            if os.path.isdir(class_path):\n",
    "                class_name = class_folder\n",
    "                #we know the label of an image based on what folder it is in\n",
    "                if class_name == \"landbirds\":\n",
    "                    class_label = 0\n",
    "                elif class_name == \"waterbirds\":\n",
    "                    class_label = 1\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                for image_filename in os.listdir(class_path):\n",
    "                    image_path = os.path.join(class_path, image_filename)\n",
    "                    self.image_paths.append(image_path)\n",
    "                    self.labels.append(class_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        #apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        filename = os.path.basename(image_path)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return image, filename, label\n",
    "    \n",
    "def preprocess(directory, batch_size, select_percentage = 100):\n",
    "    \"\"\"Transform the images in the dataset and split into training and test sets. Has the option to select less than\n",
    "            100% of the dataset during debugging phase.\"\"\"\n",
    "    transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                                   ])\n",
    "    \n",
    "    #create custom dataset with transforms\n",
    "    data = CustomDataset(data_dir=directory, transform=transform)\n",
    "\n",
    "    if select_percentage != 100:\n",
    "        data = select_fraction(data, select_percentage)\n",
    "\n",
    "    # Split the dataset into training and validation sets based on 80/20 split\n",
    "    train_size = int(0.8 * len(data))\n",
    "    val_size = len(data) - train_size\n",
    "    train_dataset, val_dataset = random_split(data, [train_size, val_size])\n",
    "\n",
    "    #we return two DataLoader objects, one for the training data and one for the test data\n",
    "    return (DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True),\n",
    "        DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False),\n",
    "        )\n",
    "\n",
    "def train_resnet50(train_loader, test_loader, model_path, epochs, learning_rate, cvar=False):\n",
    "    \"\"\"Train a ResNet50 architecture with default weights for the classification task of Waterbirds. The goal is to classify\n",
    "    an image as either waterbird or landbird. Has the option to vary the number of epochs, learning rate and the use\n",
    "    of CVaR Loss.\"\"\"\n",
    "    num_classes = 2\n",
    "    \n",
    "    #define Resnet50 architecture\n",
    "    model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "    # Freeze the layers except the last 4 layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    model.fc = nn.Sequential(nn.Linear(2048, 512),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(0.2),\n",
    "                                    nn.Linear(512, num_classes),\n",
    "                                    nn.LogSoftmax(dim=1))\n",
    "    \n",
    "    #Depending on whether we will use CVaR loss we define the criterion for the training set\n",
    "    if cvar==True:\n",
    "        criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "        cvar_loss = Loss(alpha=0.001, reg=0.01, tol=1e-4)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    #Define loss criterion for the test set\n",
    "    criterion_test = nn.CrossEntropyLoss()\n",
    "    \n",
    "    #Use stochastic gradient descent\n",
    "    optimizer = optim.SGD(model.fc.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    model.to(device)\n",
    "\n",
    "    #initialise necessary variables to keep track of training progress\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "    #option to print losses and accuracies more or less often\n",
    "    print_every = ceil(len(train_loader))\n",
    "    train_losses, test_losses, average_accuracies, group_accuracies = [], [], [], []\n",
    "    print(\"We're training with \", len(train_loader), \" batches and \", len(train_loader) * 32, \" images.\")\n",
    "    #start training\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, filenames, labels in train_loader:\n",
    "            steps += 1\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logps = model.forward(inputs)\n",
    "            loss = criterion(logps, labels)\n",
    "            if cvar == True:\n",
    "                loss = cvar_loss(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if steps % print_every == 0:\n",
    "                test_loss = 0\n",
    "                accuracy = 0\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for inputs, filenames, labels in test_loader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        logps = model.forward(inputs)\n",
    "                        batch_loss = criterion_test(logps, labels)\n",
    "                        test_loss += batch_loss.item()\n",
    "                        \n",
    "                        ps = torch.exp(logps)\n",
    "                        top_p, top_class = ps.topk(1, dim=1)\n",
    "                        equals = top_class == labels.view(*top_class.shape)\n",
    "                        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "                accuracies_per_group_dict = compute_accuracy_per_group(model, test_loader, filename_to_group_dict)\n",
    "                group_accuracies.append(accuracies_per_group_dict)\n",
    "                train_losses.append(running_loss/len(train_loader))\n",
    "                test_losses.append(test_loss/len(test_loader))\n",
    "                average_accuracies.append(accuracy/len(test_loader))\n",
    "                print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                    f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                    f\"Test loss: {test_loss/len(test_loader):.3f}.. \"\n",
    "                    f\"Test average accuracy: {accuracy/len(test_loader):.3f}..\"\n",
    "                    f\"Test group accuracies: {accuracies_per_group_dict}\")\n",
    "                running_loss = 0\n",
    "                model.train()\n",
    "    \n",
    "    #save model for analysis\n",
    "    torch.save(model, model_path)\n",
    "    \n",
    "    return train_losses, test_losses, average_accuracies, group_accuracies\n",
    "    \n",
    "def select_fraction(data, select_percentage):\n",
    "    \"Select a percentage of the dataset for faster running time during debugging.\"\n",
    "    print(\"The initial size of the data is \", len(data))\n",
    "    indices = list(range(len(data)))\n",
    "    random.shuffle(indices)\n",
    "    subset_indices = indices[:int(len(data) * (select_percentage / 100.0))]\n",
    "    return torch.utils.data.Subset(data, subset_indices)\n",
    "\n",
    "def get_misclassified_weights(model, train_loader, lambda_up):\n",
    "    \"\"\"Determine which images in the training set are misclassified by the model, and upsample these images with a \n",
    "    factor of lambda_up in the training set.\"\"\"\n",
    "    weights = []\n",
    "    with torch.no_grad():\n",
    "        for images, filenames, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "            #misclassified_mask will be true if the image is classified correctly and false if not\n",
    "            misclassified_mask = predicted_labels != labels\n",
    "            for classification in misclassified_mask:\n",
    "                if classification:\n",
    "                    weights.append(lambda_up)\n",
    "                else:\n",
    "                    weights.append(1)   \n",
    "    return weights\n",
    "\n",
    "def filename_to_group_dict(csv_file_path):\n",
    "    \"\"\"Determines the group of every image in the data using the metadata.csv file. The data can be in one of four groups: \n",
    "    landbird with land background, landbird with water background, waterbird with land background or waterbird with water\n",
    "    background. Store each filename with its corresponding group in a dictionary.\"\"\"\n",
    "    # Read the CSV file into a DataFrame\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Initialize an empty dictionary\n",
    "    filename_to_group = {}\n",
    "\n",
    "    # Iterate through the DataFrame and map conditions to values\n",
    "    for index, row in data.iterrows():\n",
    "        filename = row['img_filename']\n",
    "        last_slash_index = filename.rfind('/')\n",
    "        if last_slash_index != -1:\n",
    "            filename = filename[last_slash_index + 1:]\n",
    "        y = row['y']\n",
    "        place = row['place']\n",
    "\n",
    "        if y == 0 and place == 0:\n",
    "            filename_to_group[filename] = 0 #0 is landbird with land background\n",
    "        elif y == 0 and place == 1:\n",
    "            filename_to_group[filename] = 1 #1 is landbird with water background\n",
    "        elif y == 1 and place == 0:\n",
    "            filename_to_group[filename] = 2 #2 is waterbird with land background\n",
    "        elif y == 1 and place == 1:\n",
    "            filename_to_group[filename] = 3 #3 is waterbird with water background\n",
    "\n",
    "    return filename_to_group\n",
    "\n",
    "def compute_accuracy_per_group(model, test_loader, filename_to_group_dict):\n",
    "    \"\"\"Here we will compute the accuracies of the test set, separately for each of the four groups.\"\"\"\n",
    "    group_correct = {0:0,1:0,2:0,3:0}\n",
    "    group_total = {0:0,1:0,2:0,3:0}\n",
    "    #we will store the accuracies in a dictionary where the keys represent the groups and the values the accuracies of each group\n",
    "    group_accuracies = {}\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        images, filenames, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        for image, filename, label in zip(images, filenames, labels):\n",
    "            image = image.unsqueeze(0)\n",
    "            output = model(image)\n",
    "            group = filename_to_group_dict.get(filename)\n",
    "            #check if image was classified correctly\n",
    "            if torch.argmax(output) == label.item():\n",
    "                group_correct[group] += 1\n",
    "            group_total[group] += 1\n",
    "            \n",
    "    group_accuracies[0] = group_correct[0]/group_total[0]\n",
    "    group_accuracies[1] = group_correct[1]/group_total[1]\n",
    "    group_accuracies[2] = group_correct[2]/group_total[2]\n",
    "    group_accuracies[3] = group_correct[3]/group_total[3]\n",
    "    \n",
    "    return group_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b2c6205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're training with  295  batches and  9440  images.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ensin\\OneDrive\\Documenten\\Universiteit\\y2b1\\Research Topics\\project\\2AMM20-model\\JTT_CVaR.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ensin/OneDrive/Documenten/Universiteit/y2b1/Research%20Topics/project/2AMM20-model/JTT_CVaR.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train_loader, test_loader \u001b[39m=\u001b[39m preprocess(path_to_data ,batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m, select_percentage\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ensin/OneDrive/Documenten/Universiteit/y2b1/Research%20Topics/project/2AMM20-model/JTT_CVaR.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#training round one and visualisation of the results\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ensin/OneDrive/Documenten/Universiteit/y2b1/Research%20Topics/project/2AMM20-model/JTT_CVaR.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m train_losses, test_losses, average_accuracies, group_accuracies \u001b[39m=\u001b[39m train_resnet50(train_loader, test_loader, \u001b[39m'\u001b[39;49m\u001b[39mJTT_one\u001b[39;49m\u001b[39m'\u001b[39;49m, epochs \u001b[39m=\u001b[39;49m \u001b[39m30\u001b[39;49m, learning_rate \u001b[39m=\u001b[39;49m \u001b[39m0.001\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ensin/OneDrive/Documenten/Universiteit/y2b1/Research%20Topics/project/2AMM20-model/JTT_CVaR.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m visualisation(train_losses, test_losses, average_accuracies, group_accuracies)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ensin/OneDrive/Documenten/Universiteit/y2b1/Research%20Topics/project/2AMM20-model/JTT_CVaR.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#load the model and upsample the misclassified training images in the training dataset\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\ensin\\OneDrive\\Documenten\\Universiteit\\y2b1\\Research Topics\\project\\2AMM20-model\\JTT_CVaR.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ensin/OneDrive/Documenten/Universiteit/y2b1/Research%20Topics/project/2AMM20-model/JTT_CVaR.ipynb#W5sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m inputs, labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ensin/OneDrive/Documenten/Universiteit/y2b1/Research%20Topics/project/2AMM20-model/JTT_CVaR.ipynb#W5sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ensin/OneDrive/Documenten/Universiteit/y2b1/Research%20Topics/project/2AMM20-model/JTT_CVaR.ipynb#W5sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m logps \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(inputs)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ensin/OneDrive/Documenten/Universiteit/y2b1/Research%20Topics/project/2AMM20-model/JTT_CVaR.ipynb#W5sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(logps, labels)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ensin/OneDrive/Documenten/Universiteit/y2b1/Research%20Topics/project/2AMM20-model/JTT_CVaR.ipynb#W5sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m \u001b[39mif\u001b[39;00m cvar \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ensin\\anaconda3\\envs\\ResearchTopics\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[1;32mc:\\Users\\ensin\\anaconda3\\envs\\ResearchTopics\\lib\\site-packages\\torchvision\\models\\resnet.py:276\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    274\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[0;32m    275\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n\u001b[1;32m--> 276\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer4(x)\n\u001b[0;32m    278\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[0;32m    279\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ensin\\anaconda3\\envs\\ResearchTopics\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ensin\\anaconda3\\envs\\ResearchTopics\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ensin\\anaconda3\\envs\\ResearchTopics\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ensin\\anaconda3\\envs\\ResearchTopics\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ensin\\anaconda3\\envs\\ResearchTopics\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ensin\\anaconda3\\envs\\ResearchTopics\\lib\\site-packages\\torchvision\\models\\resnet.py:147\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    144\u001b[0m identity \u001b[39m=\u001b[39m x\n\u001b[0;32m    146\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[1;32m--> 147\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn1(out)\n\u001b[0;32m    148\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[0;32m    150\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(out)\n",
      "File \u001b[1;32mc:\\Users\\ensin\\anaconda3\\envs\\ResearchTopics\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ensin\\anaconda3\\envs\\ResearchTopics\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ensin\\anaconda3\\envs\\ResearchTopics\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[0;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[0;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    180\u001b[0m     bn_training,\n\u001b[0;32m    181\u001b[0m     exponential_average_factor,\n\u001b[0;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m    183\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ensin\\anaconda3\\envs\\ResearchTopics\\lib\\site-packages\\torch\\nn\\functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2475\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m   2476\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m-> 2478\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m   2479\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[0;32m   2480\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#execution\n",
    "\n",
    "#define path to data and create filename to group dictionary to use later\n",
    "path_to_data = 'C:/Users/ensin/OneDrive/Documenten/Universiteit/y2b1/Research Topics/project/waterbird_complete95_2class'\n",
    "filename_to_group_dict = filename_to_group_dict('C:/Users/ensin/OneDrive/Documenten/Universiteit/y2b1/Research Topics/project/waterbird_complete95_forest2water2/metadata.csv')\n",
    "\n",
    "#preprocess the data\n",
    "train_loader, test_loader = preprocess(path_to_data ,batch_size = 32, select_percentage=100)\n",
    "\n",
    "#training round one and visualisation of the results\n",
    "train_losses, test_losses, average_accuracies, group_accuracies = train_resnet50(train_loader, test_loader, 'JTT_one', epochs = 30, learning_rate = 0.001)\n",
    "visualisation(train_losses, test_losses, average_accuracies, group_accuracies)\n",
    "\n",
    "#load the model and upsample the misclassified training images in the training dataset\n",
    "model = torch.load('JTT_one')\n",
    "weights = get_misclassified_weights(model, train_loader, 20)\n",
    "sampler = WeightedRandomSampler(weights, ceil(sum(weights)), replacement=True)\n",
    "train_loader_new = DataLoader(\n",
    "            train_loader.dataset, \n",
    "            batch_size=32, \n",
    "            sampler=sampler,\n",
    "            shuffle=False)\n",
    "print(\"Now, length of train_loader is \", len(train_loader_new))\n",
    "\n",
    "#training round two with new training dataset and visualisation of the results\n",
    "train_losses2, test_losses2, average_accuracies2, group_accuracies2 = train_resnet50(train_loader_new, test_loader, 'JTT_two', epochs = 100, learning_rate = 0.0004, cvar=True)\n",
    "visualisation(train_losses2, test_losses2, average_accuracies2, group_accuracies2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
